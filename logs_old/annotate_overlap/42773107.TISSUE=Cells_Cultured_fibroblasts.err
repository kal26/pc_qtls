Building DAG of jobs...
Using shell: /home/klawren/micromamba/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Provided resources: mem_mb=20000, mem_mib=954, disk_mb=1000, disk_mib=954, nodes=1, threads=1
Select jobs to execute...

[Thu Mar 21 11:38:25 2024]
rule annotate_overlap:
    input: output/control_eqtl_proteincoding/Cells_Cultured_fibroblasts/Cells_Cultured_fibroblasts.v8.cluster_genes.susie.txt, output/pcqtl_proteincoding/Cells_Cultured_fibroblasts/Cells_Cultured_fibroblasts.v8.pcs.susie.txt
    output: output/overlap_proteincoding/Cells_Cultured_fibroblasts.v8.overlap.txt
    jobid: 0
    reason: Missing output files: output/overlap_proteincoding/Cells_Cultured_fibroblasts.v8.overlap.txt
    wildcards: TISSUE=Cells_Cultured_fibroblasts
    resources: mem_mb=20000, mem_mib=954, disk_mb=1000, disk_mib=954, tmpdir=/local/scratch/klawren/slrmtmp.42773107, partition=batch, account=smontgom, time=00:10:00, mem=20G, nodes=1, threads=1

python -c "from __future__ import print_function; import sys, json; print(json.dumps([sys.version_info.major, sys.version_info.minor]))"
Activating conda environment: tensorqtl_r
python /oak/stanford/groups/smontgom/klawren/pcqtls/.snakemake/scripts/tmpt9e696ix.get_overlap.py
Activating conda environment: tensorqtl_r
Not cleaning up /oak/stanford/groups/smontgom/klawren/pcqtls/.snakemake/scripts/tmpt9e696ix.get_overlap.py
[Thu Mar 21 11:39:12 2024]
Finished job 0.
1 of 1 steps (100%) done
