{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how often to genes in clusters share funciton?\n",
    "\n",
    "inspired by https://www.nature.com/articles/s41467-021-25129-x, \"The molecular basis, genetic control and pleiotropic effects of local gene co-expression\", espeically figure 3 and go term enrishment analysis and https://www.nature.com/articles/s42003-022-03831-w \"Shared regulation and functional relevance of local gene co-expression revealed by single cell analysis\" go term enrichment analysis\n",
    "\n",
    "* distance in groups vs not X\n",
    "* paralog frequency X \n",
    "* bidirecitonal promotors X\n",
    "* shared enhancers X \n",
    "* shared go term\n",
    "* same pathway\n",
    "* same complex \n",
    "* inverted/total ctcf motifs between\n",
    "* hi-c contacts? (from abc data?)\n",
    "* cross mappability X\n",
    "\n",
    "I want to make some plots here, and also add all of these annotations to the cluster file. At a later point, perhaps automate the annotation of the clusters file with this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from tqdm.auto import tqdm  # for notebooks\n",
    "tqdm.pandas()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import upsetplot as up\n",
    "import ast\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/klawren/oak/pcqtls/workflow/scripts')\n",
    "from residualize import calculate_residual\n",
    "\n",
    "\n",
    "# get outputs from a config file\n",
    "prefix = '/home/klawren/oak/pcqtls'\n",
    "import yaml\n",
    "config_path= f'{prefix}/config/proteincoding_main.yaml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "tissue_id_path = config['tissue_id_path']\n",
    "clusters_dir = config['clusters_dir']\n",
    "expression_dir = config['expression_dir']\n",
    "covariates_dir = config['covariates_dir']\n",
    "\n",
    "\n",
    "# load in the tissue ids \n",
    "tissue_df = pd.read_csv(f\"{prefix}/{tissue_id_path}\", header=0)\n",
    "tissue_ids = list(tissue_df['Tissue'])\n",
    "my_tissue_id = 'Thyroid'\n",
    "\n",
    "protien_coding_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all_tissues = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in data\n",
    "cluster df, expression df, gencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in gene data\n",
    "full_gencode=pd.read_csv(f'{prefix}/data/references/processed_gencode.v26.GRCh38.genes.csv')\n",
    "\n",
    "\n",
    "# filter to protien coding\n",
    "if protien_coding_only:\n",
    "    non_protein_gencode = full_gencode.copy()\n",
    "    full_gencode = full_gencode[full_gencode['gene_type'] == 'protein_coding']\n",
    "\n",
    "gid_gencode = full_gencode.set_index('transcript_id').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in cluster data for a single tissue\n",
    "cluster_df = pd.read_csv(f'{prefix}/{clusters_dir}/{my_tissue_id}_clusters_all_chr.csv', index_col=0)\n",
    "\n",
    "# load in cluster data for all tissues\n",
    "if load_all_tissues:\n",
    "    all_tissue_cluster_dfs={}\n",
    "    for tissue_id in tissue_ids: \n",
    "        all_tissue_cluster_dfs[tissue_id] = pd.read_csv(f'{prefix}/{clusters_dir}/{tissue_id}_clusters_all_chr.csv', index_col=0)\n",
    "\n",
    "# load in expression data (so we only compare to pairs of genes also expressed in this tissue)\n",
    "# load in gene expression data\n",
    "expression_df = pd.read_csv(f'{prefix}/{expression_dir}/{my_tissue_id}.v8.normalized_expression.bed', sep='\\t')\n",
    "\n",
    "# load in expression data for all tissues \n",
    "if load_all_tissues:\n",
    "    all_tissue_expression_dfs={}\n",
    "    for tissue_id in tissue_ids: \n",
    "        all_tissue_expression_dfs[tissue_id] = pd.read_csv(f'{prefix}/{expression_dir}/{tissue_id}.v8.normalized_expression.bed', sep='\\t')\n",
    "\n",
    "# expressed genes in sample tissue\n",
    "expressed_gencode = full_gencode[full_gencode['transcript_id'].isin(expression_df['gene_id'])]\n",
    "expressed_gencode = expressed_gencode.sort_values(['chr', 'start', 'end'])\n",
    "\n",
    "\n",
    "# expressed genes in all tissues\n",
    "if load_all_tissues:\n",
    "    all_tissue_expressed_gencode_dfs={}\n",
    "    for tissue_id in tissue_ids: \n",
    "        expressed_gencode = full_gencode[full_gencode['transcript_id'].isin(all_tissue_expression_dfs[tissue_id]['gene_id'])]\n",
    "        expressed_gencode = expressed_gencode.sort_values(['chr', 'start', 'end'])\n",
    "        all_tissue_expressed_gencode_dfs[tissue_id] = expressed_gencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residulized expression for correlations\n",
    "covariates_df = pd.read_csv(f'{prefix}/{covariates_dir}/{my_tissue_id}.v8.covariates.txt', sep='\\t', index_col=0).T\n",
    "gid_expression = expression_df.set_index('gene_id')[covariates_df.index]\n",
    "residal_exp = calculate_residual(gid_expression[covariates_df.index], covariates_df, center=True)\n",
    "residal_exp = pd.DataFrame(residal_exp, columns=covariates_df.index, index=gid_expression.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in ABC data for enhancer gene connections\n",
    "full_abc_pred_df = pd.read_csv(f'{prefix}/data/references/functional_annotations/ABC_predictions/AllPredictions.AvgHiC.ABC0.015.minus150.ForABCPaperV3.txt.gz', sep='\\t')\n",
    "\n",
    "# load in tissue matching for ABC-gtex tissues\n",
    "# this has to be done by hand\n",
    "# some were clear, for those that aren't I've asked for help and put None for now\n",
    "\n",
    "abc_gtex_match = pd.read_csv(f'{prefix}/data/references/functional_annotations/ABC_predictions/ABC_matched_gtex.csv')\n",
    "\n",
    "# get just the enhancer-gene connections for the matched tissue\n",
    "abc_df = full_abc_pred_df[full_abc_pred_df['CellType'] == abc_gtex_match[abc_gtex_match['GTEX_tissue'] == my_tissue_id]['ABC_biosample_id'].iloc[0]]\n",
    "\n",
    "# add transcript ids to relevant abc enhancer-gene connection columns these and set as index\n",
    "gene_enhancer_df = pd.merge(full_gencode[['transcript_id', 'gene_name']], abc_df[['TargetGene','name','class', 'ABC.Score']], left_on='gene_name', right_on='TargetGene', how='left')\n",
    "gene_enhancer_df.rename(columns={'name':'enhancer'}, inplace=True)\n",
    "gene_enhancer_df.set_index('transcript_id', inplace=True)\n",
    "gene_enhancer_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in ctcf data\n",
    "ctcf_gtex_match = pd.read_csv(f'{prefix}/data/references/functional_annotations/ctcf_chip/ctcf_matched_gtex.csv')\n",
    "ctcf_file = ctcf_gtex_match[ctcf_gtex_match['GTEX'] == my_tissue_id].iloc[0]['ctcf']\n",
    "ctcf_df = pd.read_csv(f'{prefix}/data/references/functional_annotations/ctcf_chip/{ctcf_file}.bed.gz', sep='\\t', \n",
    "                      names=['chr', 'start', 'end', 'name', 'score', 'strand', 'signal_value', 'p_value', 'q_value', 'peak'])\n",
    "ctcf_df['peak_inter'] = pd.arrays.IntervalArray.from_arrays(ctcf_df['start'], ctcf_df['end'])\n",
    "ctcf_df['Chromosome'] = ctcf_df['chr'].str.split('chr').str[1].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_positions(cluster_df)\n",
    "cluster_df['interval'] = pd.arrays.IntervalArray.from_arrays(cluster_df['start'], cluster_df['end'])\n",
    "\n",
    "# ctcf intervals for each chromosome\n",
    "chr_ctcf_peaks={}\n",
    "for chr in cluster_df['Chromosome'].unique():\n",
    "    ctcf_chr = ctcf_df[ctcf_df['Chromosome'] == chr.astype(str)]\n",
    "    chr_ctcf_peaks[chr] = pd.arrays.IntervalArray.from_arrays(ctcf_chr['start'], ctcf_chr['end'])\n",
    "\n",
    "# annotate each cluster\n",
    "for idx, row in cluster_df.iterrows():\n",
    "    num_ctcf = sum(chr_ctcf_peaks[row['Chromosome']].overlaps(row['inter']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in cluster_df.iterrows():\n",
    "    num_ctcf = sum(chr_ctcf_peaks[row['Chromosome']].overlaps(row['inter']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_positions(cluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "      <th>strand</th>\n",
       "      <th>signal_value</th>\n",
       "      <th>p_value</th>\n",
       "      <th>q_value</th>\n",
       "      <th>peak</th>\n",
       "      <th>peak_inter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chrX</td>\n",
       "      <td>10119429</td>\n",
       "      <td>10119909</td>\n",
       "      <td>.</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>1270.34860</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.08763</td>\n",
       "      <td>235</td>\n",
       "      <td>(10119429, 10119909]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr2</td>\n",
       "      <td>113603948</td>\n",
       "      <td>113604429</td>\n",
       "      <td>.</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>1160.25428</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.08763</td>\n",
       "      <td>279</td>\n",
       "      <td>(113603948, 113604429]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr10</td>\n",
       "      <td>6088699</td>\n",
       "      <td>6089084</td>\n",
       "      <td>.</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>1099.65740</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.08763</td>\n",
       "      <td>203</td>\n",
       "      <td>(6088699, 6089084]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr10</td>\n",
       "      <td>75235913</td>\n",
       "      <td>75236256</td>\n",
       "      <td>.</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>1073.99423</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.08763</td>\n",
       "      <td>169</td>\n",
       "      <td>(75235913, 75236256]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>225474967</td>\n",
       "      <td>225475276</td>\n",
       "      <td>.</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>936.04333</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>4.08763</td>\n",
       "      <td>157</td>\n",
       "      <td>(225474967, 225475276]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28014</th>\n",
       "      <td>chr8</td>\n",
       "      <td>12015181</td>\n",
       "      <td>12015491</td>\n",
       "      <td>.</td>\n",
       "      <td>712</td>\n",
       "      <td>.</td>\n",
       "      <td>8.37139</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.37080</td>\n",
       "      <td>155</td>\n",
       "      <td>(12015181, 12015491]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28015</th>\n",
       "      <td>chr2</td>\n",
       "      <td>75560266</td>\n",
       "      <td>75560576</td>\n",
       "      <td>.</td>\n",
       "      <td>616</td>\n",
       "      <td>.</td>\n",
       "      <td>8.36644</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.37025</td>\n",
       "      <td>155</td>\n",
       "      <td>(75560266, 75560576]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28016</th>\n",
       "      <td>chr10</td>\n",
       "      <td>14972126</td>\n",
       "      <td>14972436</td>\n",
       "      <td>.</td>\n",
       "      <td>1000</td>\n",
       "      <td>.</td>\n",
       "      <td>8.33594</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.36730</td>\n",
       "      <td>155</td>\n",
       "      <td>(14972126, 14972436]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28017</th>\n",
       "      <td>chr12</td>\n",
       "      <td>74538130</td>\n",
       "      <td>74538440</td>\n",
       "      <td>.</td>\n",
       "      <td>819</td>\n",
       "      <td>.</td>\n",
       "      <td>8.33145</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.36680</td>\n",
       "      <td>155</td>\n",
       "      <td>(74538130, 74538440]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28018</th>\n",
       "      <td>chr6</td>\n",
       "      <td>138773711</td>\n",
       "      <td>138774021</td>\n",
       "      <td>.</td>\n",
       "      <td>547</td>\n",
       "      <td>.</td>\n",
       "      <td>8.32663</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.36601</td>\n",
       "      <td>155</td>\n",
       "      <td>(138773711, 138774021]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28019 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         chr      start        end name  score strand  signal_value  p_value  \\\n",
       "0       chrX   10119429   10119909    .   1000      .    1270.34860     -1.0   \n",
       "1       chr2  113603948  113604429    .   1000      .    1160.25428     -1.0   \n",
       "2      chr10    6088699    6089084    .   1000      .    1099.65740     -1.0   \n",
       "3      chr10   75235913   75236256    .   1000      .    1073.99423     -1.0   \n",
       "4       chr1  225474967  225475276    .   1000      .     936.04333     -1.0   \n",
       "...      ...        ...        ...  ...    ...    ...           ...      ...   \n",
       "28014   chr8   12015181   12015491    .    712      .       8.37139     -1.0   \n",
       "28015   chr2   75560266   75560576    .    616      .       8.36644     -1.0   \n",
       "28016  chr10   14972126   14972436    .   1000      .       8.33594     -1.0   \n",
       "28017  chr12   74538130   74538440    .    819      .       8.33145     -1.0   \n",
       "28018   chr6  138773711  138774021    .    547      .       8.32663     -1.0   \n",
       "\n",
       "       q_value  peak              peak_inter  \n",
       "0      4.08763   235    (10119429, 10119909]  \n",
       "1      4.08763   279  (113603948, 113604429]  \n",
       "2      4.08763   203      (6088699, 6089084]  \n",
       "3      4.08763   169    (75235913, 75236256]  \n",
       "4      4.08763   157  (225474967, 225475276]  \n",
       "...        ...   ...                     ...  \n",
       "28014  0.37080   155    (12015181, 12015491]  \n",
       "28015  0.37025   155    (75560266, 75560576]  \n",
       "28016  0.36730   155    (14972126, 14972436]  \n",
       "28017  0.36680   155    (74538130, 74538440]  \n",
       "28018  0.36601   155  (138773711, 138774021]  \n",
       "\n",
       "[28019 rows x 11 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctcf_df['peak_inter'] = pd.arrays.IntervalArray.from_arrays(ctcf_df['start'], ctcf_df['end'])\n",
    "ctcf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in paralogs\n",
    "paralog_df = pd.read_csv(f'{prefix}/data/references/functional_annotations/paralogs_biomart_ensembl97.tsv.gz', sep='\\t')\n",
    "\n",
    "# drop genes that don't have paralogues\n",
    "paralog_df.dropna(subset=['Human paralogue gene stable ID'], inplace=True)\n",
    "\n",
    "# group by the gene that has the paralogs (this is bidirectional)\n",
    "# this needs to be done on gene ids without versions, as this isn't the same version number (sadly the correct version isn't availble on biomart)\n",
    "paralog_df = paralog_df.groupby('Gene stable ID')['Human paralogue gene stable ID'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in go terms\n",
    "go_df = pd.read_csv(f'{prefix}/data/references/functional_annotations/go_biomart_ensembl97.tsv.gz', sep='\\t', header=None,\n",
    "                    names = ['Gene stable ID', 'Gene stable ID version', 'Gene start (bp)', 'Gene end', 'Strand', 'tss', 'gencode_annotation', 'gene_name', 'transcript_type', 'go_accession', 'go_name', 'go_domain'])\n",
    "\n",
    "# only consider matching biological process go terms, as in Ribiero 2021\n",
    "go_df = go_df[go_df['go_domain'] == 'biological_process'].groupby('Gene stable ID')['go_accession'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in cross mapablity (this is cleaned up in cross_mappability.ipynb)\n",
    "cross_mappability = pd.read_csv(f'{prefix}/data/references/cross_mappability/cross_mappability_100_agg.csv')\n",
    "cross_mappability.set_index('gene_1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in complexes\n",
    "complex_df = pd.read_csv('/home/klawren/oak/pcqtls/data/references/functional_annotations/humancomplexes_corum4.1.txt', sep='\\t')\n",
    "\n",
    "# add ensemble IDs based on gene names, to end with a ensemble ID indexed list of complexes\n",
    "complex_df['all_subunits'] = complex_df['subunits(Gene name syn)'].astype(str) +  ';' + complex_df['subunits(Gene name)'].astype(str)\n",
    "complex_df['all_subunit_gene_names'] = complex_df['all_subunits'].str.split(';| ')\n",
    "complex_df = full_gencode[['transcript_id', 'gene_name']].join(complex_df.explode('all_subunit_gene_names').groupby('all_subunit_gene_names').agg({'ComplexID':set}), on='gene_name')\n",
    "complex_df.set_index('transcript_id', inplace=True)\n",
    "complex_df.drop('gene_name', inplace=True, axis=1)\n",
    "complex_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## annotation functions for clusters\n",
    "\n",
    "#### bidirectional promotors\n",
    "if two of the genes in the cluster are opposite strand with tss < 1000 bp away, classify as bidirecitonal promotor\n",
    "\n",
    "#### Enhancer sharing \n",
    "\n",
    "ABC for 131 biosmaples (could probably find matches for most of mine) from Nasser et al. Nature (2021): Genome-wide enhancer maps connect risk variants to disease genes. (download link: https://www.engreitzlab.org/resources) copied from /oak/stanford/groups/engreitz/public/Nasser2021\n",
    "\n",
    "#### GO term sharing, paralogs\n",
    "\n",
    "downloaded all go terms, paralogs for ensemble IDs from biomart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cluster size from gene-gene into cluster df\n",
    "def get_cluster_size(row):\n",
    "    transcript_ids = row['Transcripts'].split(',')\n",
    "    cluster_gencode = gid_gencode.loc[transcript_ids]\n",
    "    return  cluster_gencode['end'].max() - cluster_gencode['start'].min()\n",
    "\n",
    "def get_cluster_tss_size(row):\n",
    "    transcript_ids = row['Transcripts'].split(',')\n",
    "    cluster_gencode = gid_gencode.loc[transcript_ids]\n",
    "    return  cluster_gencode['tss_start'].max() - cluster_gencode['tss_start'].min()\n",
    "\n",
    "def get_cluster_start_ids(cluster_df):\n",
    "    # the first for pairs, the first and second for threes, ect\n",
    "    cluster_start_ids = []\n",
    "    for i in range(cluster_df['N_genes'].max()):\n",
    "        out_ids = cluster_df[cluster_df['N_genes'] == i]['Transcripts'].str.split(',').str[:i-1].values\n",
    "        if len(out_ids)>0:\n",
    "            cluster_start_ids.append(np.concatenate(out_ids))\n",
    "        else:\n",
    "            cluster_start_ids.append([])\n",
    "    return cluster_start_ids\n",
    "\n",
    "def annotate_sizes(cluster_df):\n",
    "    cluster_df['cluster_size'] = cluster_df.progress_apply(get_cluster_size, axis=1)\n",
    "    cluster_df['cluster_tss_size'] = cluster_df.progress_apply(get_cluster_tss_size, axis=1)\n",
    "\n",
    "def annotate_positions(cluster_df):\n",
    "    for idx, row in cluster_df.iterrows():\n",
    "        transcript_ids = row['Transcripts'].split(',')\n",
    "        cluster_gencode = gid_gencode.loc[transcript_ids]\n",
    "        cluster_df.loc[idx, 'start'] = cluster_gencode['start'].min()\n",
    "        cluster_df.loc[idx, 'end'] = cluster_gencode['end'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bidirectional(row, gid_gencode):\n",
    "    transcript_ids = row['Transcripts'].split(',')\n",
    "    cluster_gencode = gid_gencode.loc[transcript_ids]\n",
    "    num_bidirectional = 0\n",
    "    # check all pairwise combos of genes\n",
    "    for idx, first_gene_row in cluster_gencode.iterrows():\n",
    "        for idx, second_gene_row in cluster_gencode.iterrows():\n",
    "            opp_strand = first_gene_row['strand'] != second_gene_row['strand']\n",
    "            close = abs(first_gene_row['tss_start'] - second_gene_row['tss_start']) <= 1000\n",
    "            if opp_strand & close:\n",
    "                # found a bidirectional promotor\n",
    "                num_bidirectional +=1\n",
    "\n",
    "    # didn't find a bidirectional promotor\n",
    "    return num_bidirectional/2\n",
    "\n",
    "def annotate_bidirectional(cluster_df, gid_gencode):\n",
    "    cluster_df['num_bidirectional_promoter'] = cluster_df.progress_apply(get_bidirectional, axis=1, args=(gid_gencode,))\n",
    "    cluster_df['has_bidirectional_promoter'] = cluster_df['num_bidirectional_promoter'] > 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate clusters with the number of shared enhancers\n",
    "\n",
    "def annotate_enhancers(cluster_df, gene_enhancer_df):\n",
    "    for idx, row in tqdm(cluster_df.iterrows(), total=len(cluster_df)):\n",
    "        enhancer_list = gene_enhancer_df[gene_enhancer_df.index.isin(row['Transcripts'].split(','))]\n",
    "        full_enhancer_list = enhancer_list['enhancer']\n",
    "        strong_enhancer_list = enhancer_list[enhancer_list['ABC.Score']>=0.1]['enhancer']\n",
    "        very_strong_enhancer_list = enhancer_list[enhancer_list['ABC.Score']>=0.25]['enhancer']\n",
    "\n",
    "        num_shared_enhancers = sum(full_enhancer_list.duplicated())\n",
    "        num_shared_strong_enhancers = sum(strong_enhancer_list.duplicated())\n",
    "        num_shared_very_strong_enhancers = sum(very_strong_enhancer_list.duplicated())\n",
    "\n",
    "\n",
    "        cluster_df.loc[idx, 'num_shared_enhancers'] = num_shared_enhancers\n",
    "        cluster_df.loc[idx, 'num_shared_strong_enhancers'] = num_shared_strong_enhancers\n",
    "        cluster_df.loc[idx, 'num_enhancers'] = len(full_enhancer_list)\n",
    "        cluster_df.loc[idx, 'num_strong_enhancers'] = len(strong_enhancer_list)\n",
    "        cluster_df.loc[idx, 'has_shared_enhancer'] = num_shared_enhancers > 0\n",
    "        cluster_df.loc[idx, 'has_shared_strong_enhancer'] = num_shared_strong_enhancers > 0\n",
    "        cluster_df.loc[idx, 'has_shared_very_strong_enhancer'] = num_shared_very_strong_enhancers > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_ctcf(cluster_df, ctcf_df):\n",
    "    annotate_positions(cluster_df)\n",
    "    cluster_df['interval'] = pd.arrays.IntervalArray.from_arrays(cluster_df['start'], cluster_df['end'])\n",
    "\n",
    "    # ctcf intervals for each chromosome\n",
    "    chr_ctcf_peaks={}\n",
    "    for chr in cluster_df['Chromosome'].unique():\n",
    "        ctcf_chr = ctcf_df[ctcf_df['Chromosome'] == chr.astype(str)]\n",
    "        chr_ctcf_peaks[chr] = pd.arrays.IntervalArray.from_arrays(ctcf_chr['start'], ctcf_chr['end'])\n",
    "\n",
    "    # annotate each cluster\n",
    "    for idx, row in tqdm(cluster_df.iterrows(), total=len(cluster_df)):\n",
    "        num_ctcf = sum(chr_ctcf_peaks[row['Chromosome']].overlaps(row['inter']))\n",
    "        cluster_df.loc[idx, 'num_ctcf'] = num_ctcf\n",
    "        cluster_df.loc[idx, 'has_ctcf'] = num_ctcf > 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e730cc9c19480e9859ac46b0adc920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "annotate_ctcf(cluster_df, ctcf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def annotate_enhancers_jaccard(cluster_df, gene_enhancer_df):\n",
    "    for idx, row in tqdm(cluster_df.iterrows(), total=len(cluster_df)):\n",
    "        transcript_list = row['Transcripts'].split(',')\n",
    "        jaccards_unweighted=[]\n",
    "        jaccards_weighted=[]\n",
    "        for i in range(len(transcript_list)):\n",
    "            for j in range(i):\n",
    "                enhancer_list = gene_enhancer_df[gene_enhancer_df.index.isin([transcript_list[i], transcript_list[j]])]\n",
    "                enhancer_list['ABC.Score_min'] = enhancer_list['ABC.Score']\n",
    "                enhancer_min_max = enhancer_list.groupby('enhancer').agg({'ABC.Score':'max', 'ABC.Score_min':'min'})\n",
    "\n",
    "                # zero out the mins for those elements that exist for only 1 gene\n",
    "                enhancer_gene_counts = enhancer_list.groupby('enhancer').agg({'gene_name':'nunique'}) \n",
    "                single_enhancers = enhancer_gene_counts.index.values[enhancer_gene_counts['gene_name'] == 1]\n",
    "                enhancer_min_max.loc[single_enhancers, 'ABC.Score_min'] = 0\n",
    "                # jaccard without reweighting\n",
    "                jaccards_unweighted.append(enhancer_min_max['ABC.Score_min'].sum()/enhancer_min_max['ABC.Score'].sum())\n",
    "\n",
    "                # assuming these don't sum to 1 for a given gene becuase the promoter-self connections aren't listed\n",
    "                # add in an element for each genes promotor to get the final weighting right\n",
    "                reweightings = enhancer_list.groupby('gene_name').agg({'ABC.Score':sum})\n",
    "                reweightings['ABC.Score'] = 1- reweightings['ABC.Score']\n",
    "                reweightings['ABC.Score_min'] = 0\n",
    "\n",
    "                enhancer_min_max = pd.concat([enhancer_min_max, reweightings])\n",
    "                # jaccard with reweighting\n",
    "                jaccards_weighted.append(enhancer_min_max['ABC.Score_min'].sum()/enhancer_min_max['ABC.Score'].sum())\n",
    "\n",
    "        #jaccards_unweighted = np.nan_to_num(jaccards_unweighted)\n",
    "        #jaccards_weighted = np.nan_to_num(jaccards_weighted)\n",
    "\n",
    "        cluster_df.loc[idx, 'max_jaccard_unweighted'] = max(jaccards_unweighted)\n",
    "        cluster_df.loc[idx, 'max_jaccard_weighted'] = max(jaccards_weighted)\n",
    "        cluster_df.loc[idx, 'has_high_jaccard_unweighted'] = max(jaccards_unweighted) > 0.5\n",
    "        cluster_df.loc[idx, 'has_high_jaccard_weighted'] = max(jaccards_weighted) > 0.5\n",
    "        cluster_df.loc[idx, 'mean_jaccard_unweighted'] = np.average(jaccards_unweighted)\n",
    "        cluster_df.loc[idx, 'mean_jaccard_weighted'] = np.average(jaccards_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'max_jaccard_weighted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/tensorqtl_r/lib/python3.8/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/micromamba/envs/tensorqtl_r/lib/python3.8/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/micromamba/envs/tensorqtl_r/lib/python3.8/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'max_jaccard_weighted'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sns\u001b[38;5;241m.\u001b[39mscatterplot(cluster_df[\u001b[38;5;241m~\u001b[39m\u001b[43mcluster_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_jaccard_weighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna()], y\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_jaccard_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_jaccard_unweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/tensorqtl_r/lib/python3.8/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/micromamba/envs/tensorqtl_r/lib/python3.8/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'max_jaccard_weighted'"
     ]
    }
   ],
   "source": [
    "sns.scatterplot(cluster_df[~cluster_df['max_jaccard_weighted'].isna()], y='max_jaccard_weighted', x='max_jaccard_unweighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotated_correlation(cluster_df, residal_exp):\n",
    "    for idx, row in tqdm(cluster_df.iterrows(), total=len(cluster_df)):\n",
    "        transcript_list = row['Transcripts'].split(',')\n",
    "        cluster_expression = residal_exp.loc[transcript_list].T.corr('spearman').to_numpy()\n",
    "        cluster_corr = cluster_expression[np.triu_indices(len(cluster_expression), k=1)]\n",
    "        cluster_df.loc[idx, 'Mean_cor'] = cluster_corr.mean()\n",
    "        cluster_df.loc[idx, 'Mean_pos_cor'] = cluster_corr[cluster_corr>1].mean()\n",
    "        cluster_df.loc[idx, 'Mean_neg_cor'] = cluster_corr[cluster_corr<1].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_go(cluster_df, go_df):\n",
    "    for idx, row in tqdm(cluster_df.iterrows(), total=len(cluster_df)):\n",
    "        transcript_list_versions = row['Transcripts'].split(',')\n",
    "        transcript_list_no_versions = [transcript.split('.')[0] for transcript in transcript_list_versions]\n",
    "\n",
    "        go_list = go_df[go_df.index.isin(transcript_list_no_versions)]\n",
    "        num_shared_go_all = sum(go_list.duplicated())\n",
    "        # number genes that share all their go terms with another gene\n",
    "        cluster_df.loc[idx, 'num_shared_go_all'] = num_shared_go_all\n",
    "        cluster_df.loc[idx, 'has_shared_go_all'] = num_shared_go_all > 0\n",
    "\n",
    "        # number go terms shared between any genes\n",
    "        num_shared_go_any = sum(go_list.explode().duplicated())\n",
    "        cluster_df.loc[idx, 'num_shared_go_any'] = num_shared_go_any\n",
    "        cluster_df.loc[idx, 'has_shared_go_any'] = num_shared_go_any > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_complexes(cluster_df, complex_df):\n",
    "    for idx, row in tqdm(cluster_df.iterrows(), total=len(cluster_df)):\n",
    "        complex_list = complex_df[complex_df.index.isin(row['Transcripts'].split(','))]\n",
    "        num_complexes = sum(complex_list.explode('ComplexID').duplicated())\n",
    "        cluster_df.loc[idx, 'num_complexes'] = num_complexes\n",
    "        cluster_df.loc[idx, 'has_complexes'] = num_complexes > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotate paralogs\n",
    "def get_paralogs(row, paralog_df):\n",
    "    transcript_list_versions = row['Transcripts'].split(',')\n",
    "    transcript_list_no_versions = set([transcript.split('.')[0] for transcript in transcript_list_versions])\n",
    "\n",
    "    paralogs = 0\n",
    "    for transcript in transcript_list_no_versions:\n",
    "        try:\n",
    "            has_paralog = bool(paralog_df.loc[transcript] & transcript_list_no_versions)\n",
    "            paralogs += has_paralog\n",
    "        except KeyError:\n",
    "            # if this isn't in the paralog df, it has no paralogs, so continue\n",
    "            pass\n",
    "    return paralogs\n",
    "\n",
    "def annotate_paralogs(cluster_df, paralog_df):\n",
    "    cluster_df['num_paralog'] = cluster_df.progress_apply(get_paralogs, axis=1, args=(paralog_df,))\n",
    "    cluster_df['has_paralog'] = cluster_df['num_paralog'] > 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_map(row, cross_mappability, cross_map_threshold=100):\n",
    "    # number of transcripts that cross map to some other transcript in the cluster\n",
    "    transcript_list = set(row['Transcripts'].split(','))\n",
    "    cross_maps = 0\n",
    "    for transcript in transcript_list:\n",
    "        try:\n",
    "            cross_map_this_transcript = cross_mappability.loc[transcript]\n",
    "            pass_threshold_mask = np.asarray(ast.literal_eval(cross_map_this_transcript['cross_mappability'])) > cross_map_threshold\n",
    "            cross_map_this_transcript = np.asarray(ast.literal_eval(cross_map_this_transcript['gene_2_full']))[pass_threshold_mask]\n",
    "            has_cross_map = bool(set(cross_map_this_transcript) & transcript_list)\n",
    "            cross_maps += has_cross_map\n",
    "        except KeyError:\n",
    "            # if this isn't in the paralog df, it has no paralogs, so continue\n",
    "            pass\n",
    "    return cross_maps\n",
    "\n",
    "def annotate_cross_maps(cluster_df, cross_mappability):\n",
    "    cluster_df['num_cross_map'] = cluster_df.progress_apply(get_cross_map, axis=1, args=(cross_mappability,))\n",
    "    cluster_df['has_cross_map'] = cluster_df['num_cross_map'] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotate a cluster df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df['has_neg_corr'] = ~cluster_df['Mean_neg_cor'].isna()\n",
    "cluster_df['has_high_pos_corr'] = cluster_df['Mean_pos_cor'] > .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_annotations(cluster_df):\n",
    "    annotate_sizes(cluster_df)\n",
    "    annotate_bidirectional(cluster_df, gid_gencode)\n",
    "    annotate_enhancers(cluster_df, gene_enhancer_df)\n",
    "    annotate_paralogs(cluster_df, paralog_df)\n",
    "    annotate_cross_maps(cluster_df,cross_mappability)\n",
    "    annotate_go(cluster_df, go_df)\n",
    "    annotate_complexes(cluster_df, complex_df)\n",
    "    annotate_enhancers_jaccard(cluster_df, gene_enhancer_df)\n",
    "\n",
    "add_annotations(cluster_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot overlap between categores in the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bool_indexed(cluster_df, column_list):\n",
    "    upset_cluster_df = cluster_df.set_index(column_list[0])\n",
    "    for column_name in column_list[1:]:\n",
    "        upset_cluster_df.set_index(column_name, append=True, inplace=True)\n",
    "    return upset_cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deviation(cluster_df, column_list, type='count'):\n",
    "    bool_indexed_clusters = generate_bool_indexed(cluster_df,column_list)\n",
    "    f_observed = up.query(bool_indexed_clusters).subset_sizes\n",
    "\n",
    "    category_totals = up.query(bool_indexed_clusters).category_totals\n",
    "    inverse_category_totals = len(cluster_df) - category_totals\n",
    "\n",
    "    f_expected = f_observed.copy()\n",
    "\n",
    "    for idx, row in pd.DataFrame(f_expected).iterrows():\n",
    "        # select the marginal totals corresponding to this index\n",
    "        values_from_true_index = category_totals[list(idx)].values\n",
    "        values_from_false_index = inverse_category_totals[[not i for i in idx]].values\n",
    "        marginal_values = np.concatenate([values_from_false_index, values_from_true_index])\n",
    "        marginal_percents = marginal_values/len(cluster_df)\n",
    "        # set the value\n",
    "        f_expected.loc[idx] = np.prod(marginal_percents) * len(cluster_df)\n",
    "\n",
    "    # could do this as percent, but that emphasizs small cats\n",
    "    if type=='percent':\n",
    "        return (f_observed - f_expected) / f_expected * 100 \n",
    "    elif type=='count':\n",
    "        return (f_observed - f_expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_upset_binary_categories(cluster_df, column_list, min_subset_size=5, plot_deviation=True):\n",
    "    bool_indexed_clusters = generate_bool_indexed(cluster_df, column_list)\n",
    "    deviation = get_deviation(cluster_df, column_list)\n",
    "\n",
    "    bool_indexed_with_deviation = bool_indexed_clusters.join(deviation.rename('deviation'))\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 5))\n",
    "    upset = up.UpSet(bool_indexed_with_deviation, show_counts=True, min_subset_size=min_subset_size, sort_by='cardinality')\n",
    "    if plot_deviation:\n",
    "        upset.add_catplot(kind='bar', value='deviation', width=.6, color='k')\n",
    "    upset.plot(fig=fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_upset_binary_categories(cluster_df, ['has_cross_map', 'has_paralog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_upset_binary_categories(cluster_df, ['has_bidirectional_promoter', 'has_shared_enhancer', 'has_paralog', 'has_cross_map', 'has_neg_corr', 'has_high_pos_corr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nulls for each tissue, at each cluster size\n",
    "\n",
    "maybe subsample these to make them smaller? \n",
    "\n",
    "Should they be genome wide, or all the ones not in the actual clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cluster size from gene-gene into cluster df\n",
    "def get_cluster_size(row):\n",
    "    transcript_ids = row['Transcripts'].split(',')\n",
    "    cluster_gencode = gid_gencode.loc[transcript_ids]\n",
    "    return  cluster_gencode['end'].max() - cluster_gencode['start'].min()\n",
    "\n",
    "def get_cluster_tss_size(row):\n",
    "    transcript_ids = row['Transcripts'].split(',')\n",
    "    cluster_gencode = gid_gencode.loc[transcript_ids]\n",
    "    return  cluster_gencode['tss_start'].max() - cluster_gencode['tss_start'].min()\n",
    "\n",
    "def get_cluster_start_ids(cluster_df):\n",
    "    # the first for pairs, the first and second for threes, ect\n",
    "    cluster_start_ids = []\n",
    "    for i in range(cluster_df['N_genes'].max()):\n",
    "        out_ids = cluster_df[cluster_df['N_genes'] == i]['Transcripts'].str.split(',').str[:i-1].values\n",
    "        if len(out_ids)>0:\n",
    "            cluster_start_ids.append(np.concatenate(out_ids))\n",
    "        else:\n",
    "            cluster_start_ids.append([])\n",
    "    return cluster_start_ids\n",
    "\n",
    "def annotate_sizes(cluster_df):\n",
    "    cluster_df['cluster_size'] = cluster_df.progress_apply(get_cluster_size, axis=1)\n",
    "    cluster_df['cluster_tss_size'] = cluster_df.progress_apply(get_cluster_tss_size, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_null_clusters(expressed_gencode, cluster_size, cluster_df=None):\n",
    "    # sort\n",
    "    expressed_gencode = expressed_gencode.sort_values(['chr', 'start', 'end'])\n",
    "    # on a per chrom basis\n",
    "    null_cluster_dfs = []\n",
    "    for chr_id in range (1,23,1):\n",
    "        chr_subset_gencode = expressed_gencode[expressed_gencode['chr'] == f'chr{chr_id}']\n",
    "        transcripts = chr_subset_gencode['transcript_id'].astype(str) + ',' + chr_subset_gencode['transcript_id'].shift(-(cluster_size-1)).astype(str)\n",
    "        chr_sizes = chr_subset_gencode['end'].shift(-(cluster_size-1)) - chr_subset_gencode['start']\n",
    "\n",
    "        # trim off the blanks created from shifting\n",
    "        transcripts = transcripts.iloc[:-(cluster_size-1)]\n",
    "        chr_sizes = chr_sizes.iloc[:-(cluster_size-1)]\n",
    "\n",
    "        # select those that are not already clusters\n",
    "        try:\n",
    "            cluster_start_ids = get_cluster_start_ids(cluster_df)\n",
    "            this_cluster_size_start_ids = np.concatenate(cluster_start_ids[cluster_size:])\n",
    "            in_cluster_bool = pd.Series(chr_subset_gencode['transcript_id'].iloc[:-(cluster_size-1)]).isin(this_cluster_size_start_ids).values\n",
    "            null_cluster_dfs.append(pd.DataFrame({'Transcripts':transcripts[~in_cluster_bool], 'cluster_size':chr_sizes[~in_cluster_bool], 'chr':chr_id}))\n",
    "        except TypeError:\n",
    "            # no subselection wanted, i.e.None passed for cluster_df\n",
    "            null_cluster_dfs.append(pd.DataFrame({'Transcripts':transcripts, 'cluster_size':chr_sizes, 'chr':chr_id}))\n",
    "\n",
    "    null_df = pd.concat(null_cluster_dfs)\n",
    "    null_df.reset_index(drop=True, inplace=True)\n",
    "    return null_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from ben\n",
    "# target has to be smaller or the two, n is length of target\n",
    "\n",
    "def resample_dist(target, candidate_pool, n, seed=126124):   \n",
    "    \"\"\"Match a target distribution via weighted sampling from a candidate pool\n",
    "    Args:\n",
    "        target, candidate_pool: 1D numpy arrays with values ranging from 0 to 1\n",
    "        n: integer number of indices to return\n",
    "    Return:\n",
    "        n indices to elements candidate_pool to use for the sample\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    target_prob = sp.stats.gaussian_kde(target)\n",
    "    candidate_prob = sp.stats.gaussian_kde(candidate_pool)\n",
    "\n",
    "    bins = np.arange(0, 1, 0.0001)\n",
    "    sampling_weight = target_prob(bins) / candidate_prob(bins)\n",
    "    pool_bins = np.searchsorted(bins, candidate_pool) - 1\n",
    "    pool_probability = sampling_weight[pool_bins]/sampling_weight[pool_bins].sum()\n",
    "\n",
    "    return rng.choice(candidate_pool.size, size=n, replace=True, p=pool_probability)\n",
    "\n",
    "\n",
    "def get_resamp_null_cluster(null_df, cluster_df, plot=False, number_null = 5000):\n",
    "    # note that cluster_df should be resticted only to clusters with a matching number of transcripts already\n",
    "\n",
    "    join_df = pd.concat([cluster_df, null_df], keys=['cluster', 'null'], names=['is_cluster', 'id'])\n",
    "    if plot:\n",
    "        # size distribution before\n",
    "        sns.kdeplot(join_df.reset_index(), x='cluster_tss_size', hue='is_cluster', bw_adjust=.3, common_norm=False, log_scale=(10,None))\n",
    "        plt.title('Distance distribution before resampling')\n",
    "        plt.show()\n",
    "\n",
    "    # add a normalized cluster size column to resample on\n",
    "    cluster_df['normed_log_size'] = np.log10(cluster_df['cluster_tss_size'])/np.log10(join_df['cluster_tss_size'].max())\n",
    "    null_df['normed_log_size'] = np.log10(null_df['cluster_tss_size'])/np.log10(join_df['cluster_tss_size'].max())\n",
    "\n",
    "    # resample to match distance\n",
    "    resamp_idxs = resample_dist(cluster_df['normed_log_size'], null_df['normed_log_size'], n=number_null)\n",
    "    resamp_null_df = null_df.reset_index().iloc[resamp_idxs]\n",
    "\n",
    "    if plot:\n",
    "        # size distribution after resampling\n",
    "        join_df = pd.concat([cluster_df, resamp_null_df], keys=['cluster', 'null'], names=['is_cluster', 'id'])\n",
    "        sns.kdeplot(join_df, x='cluster_tss_size', hue='is_cluster', bw_adjust=.3, common_norm=False, log_scale=(10,None))\n",
    "        plt.title('Distance distribution after resampling')\n",
    "        plt.show()\n",
    "\n",
    "    return resamp_null_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_pairs_genome = get_null_clusters(expressed_gencode, 2, cluster_df=None)\n",
    "add_annotations(null_pairs_genome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance matched null, excluding clusters\n",
    "null_pairs_exclude_clusters = get_null_clusters(expressed_gencode, 2, cluster_df=cluster_df)\n",
    "annotate_sizes(cluster_df)\n",
    "annotate_sizes(null_pairs_exclude_clusters)\n",
    "null_pairs_genome_dist_matched = get_resamp_null_cluster(null_pairs_exclude_clusters, cluster_df[cluster_df['N_genes']==2], plot=True)\n",
    "add_annotations(null_pairs_genome_dist_matched)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_enhancers_jaccard(cluster_df, gene_enhancer_df)\n",
    "null_pairs_exclude_clusters = get_null_clusters(expressed_gencode, 2, cluster_df=cluster_df)\n",
    "annotate_enhancers_jaccard(null_pairs_exclude_clusters, gene_enhancer_df)\n",
    "annotated_correlation(null_pairs_exclude_clusters, residal_exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = pd.concat([cluster_df[cluster_df['N_genes']==2], null_pairs_exclude_clusters], keys=['cluster', 'null'], names=['type', 'idx'])\n",
    "sns.scatterplot(joined_df[~joined_df['mean_jaccard_unweighted'].isna()], x='Mean_cor', y='mean_jaccard_unweighted', hue='type')\n",
    "plt.show()\n",
    "sns.scatterplot(joined_df[~joined_df['mean_jaccard_weighted'].isna()], x='Mean_cor', y='mean_jaccard_weighted', hue='type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(joined_df[~joined_df['mean_jaccard_unweighted'].isna()]['mean_jaccard_unweighted'], joined_df[~joined_df['mean_jaccard_unweighted'].isna()]['Mean_cor'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot odds ratios\n",
    "\n",
    "def get_odds_ratio(contingency_table, verb=0):\n",
    "    # One-sided Fisher's exact test\n",
    "    odds_ratio, p_value = stats.fisher_exact(contingency_table, alternative='greater')\n",
    "    if verb > 0:\n",
    "        print(\"Odds Ratio:\", odds_ratio)\n",
    "        print(\"P-value:\", p_value)\n",
    "\n",
    "    # Compute 95% confidence interval for odds ratio\n",
    "    a, b, c, d = contingency_table.flatten()\n",
    "    SE = np.sqrt(1/a + 1/b + 1/c + 1/d)\n",
    "    if odds_ratio==0:\n",
    "        lcb=0\n",
    "        ucb=0\n",
    "    else:\n",
    "        lcb = math.exp(math.log(odds_ratio) - 1.96*SE)  # lower confidence bound\n",
    "        ucb = math.exp(math.log(odds_ratio) + 1.96*SE)  # upper confidence bound\n",
    "    if verb > 0:\n",
    "        print(f\"CI: [{lcb}, {ucb}]\")\n",
    "\n",
    "    return p_value, odds_ratio, lcb, ucb\n",
    "\n",
    "def get_contingency_table(cluster_df, null_df, column_name):\n",
    "    num_cluster = sum(cluster_df[column_name])\n",
    "    num_null = sum(null_df[column_name])\n",
    "    # Build contingency table\n",
    "    # [[yes clusters, no clusters] [yes null, no null]]\n",
    "    contingency_table = np.array([[num_cluster, len(cluster_df)-num_cluster],[num_null, len(null_df)-num_null]])\n",
    "    return contingency_table\n",
    "\n",
    "def get_log_odds(cluster_df, null_df, column_list):\n",
    "    log_odds_df = []\n",
    "    for column_name in column_list:\n",
    "        p_value, odds_ratio, lcb, ucb = get_odds_ratio(get_contingency_table(cluster_df, null_df, column_name))\n",
    "        log_odds_df.append(pd.Series({'p_value': p_value,\n",
    "                'odds_ratio':odds_ratio,\n",
    "                'lower_cb':lcb,\n",
    "                'upper_cb':ucb,\n",
    "                'lower_cb_diff': odds_ratio - lcb,\n",
    "                'upper_cb_diff': ucb - odds_ratio,\n",
    "                'col':column_name}))\n",
    "    return pd.DataFrame(log_odds_df)\n",
    "\n",
    "\n",
    "# percent belonging to category\n",
    "def get_frac(column_name, df):\n",
    "    return df[column_name].sum()/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_odds(cluster_df, null_df, column_list):\n",
    "    log_odds_df = get_log_odds(cluster_df,null_df, column_list)\n",
    "\n",
    "    log_odds_df['frac_cluster'] = log_odds_df['col'].apply(get_frac, args=(cluster_df,))\n",
    "    log_odds_df['frac_null'] = log_odds_df['col'].apply(get_frac, args=(null_pairs_genome,))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10,6))\n",
    "\n",
    "    # log odds plot\n",
    "    axes[0].errorbar(y=log_odds_df['col'], x=log_odds_df['odds_ratio'], xerr=log_odds_df[['lower_cb_diff', 'upper_cb_diff']].values.transpose(), fmt=\"o\")\n",
    "    axes[0].axvline(1, color='k', linestyle='--')\n",
    "    axes[0].set_xlabel('Log odds')\n",
    "\n",
    "    for idx,row in log_odds_df.iterrows():\n",
    "        axes[0].annotate('p={:.1E}'.format(row['p_value']), (row['odds_ratio'], idx+.05))\n",
    "        axes[0].annotate('OR={:.1f}'.format(row['odds_ratio']), (row['odds_ratio'], idx-.25))\n",
    "\n",
    "\n",
    "    # fraction plot\n",
    "    sns.barplot(data=log_odds_df, x='frac_cluster', y='col', ax=axes[1])\n",
    "    axes[1].set_yticklabels([])\n",
    "    axes[1].set_ylabel('')\n",
    "    axes[1].set_xlabel('Fraction in category')\n",
    "\n",
    "    # make the labels match in order and position\n",
    "    axes[1].invert_yaxis()\n",
    "    axes[0].set_ylim(axes[1].get_ylim())\n",
    "\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_odds(cluster_df[cluster_df['N_genes']==2], null_pairs_genome, ['has_bidirectional_promoter', 'has_shared_enhancer', 'has_shared_strong_enhancer', 'has_shared_very_strong_enhancer', 'has_paralog', 'has_cross_map', 'has_shared_go_any', 'has_shared_go_all', 'has_complexes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plot_log_odds(cluster_df[(cluster_df['N_genes']==2)&(cluster_df['has_cross_map']==False)], null_pairs_genome[null_pairs_genome['has_cross_map']==False], ['has_bidirectional_promoter', 'has_shared_enhancer', 'has_shared_strong_enhancer', 'has_shared_very_strong_enhancer', 'has_paralog', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "axes[0].set_title('Excluding cross mappable genes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plot_log_odds(cluster_df[cluster_df['N_genes']==2], null_pairs_genome_dist_matched, ['has_bidirectional_promoter', 'has_shared_enhancer', 'has_shared_strong_enhancer', 'has_shared_very_strong_enhancer', 'has_paralog', 'has_cross_map', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "axes[0].set_title('Distance matched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plot_log_odds(cluster_df[(cluster_df['N_genes']==2)&(cluster_df['has_cross_map']==False)], null_pairs_genome_dist_matched[(null_pairs_genome_dist_matched['has_cross_map']==False)], ['has_bidirectional_promoter', 'has_shared_strong_enhancer', 'has_shared_very_strong_enhancer', 'has_paralog', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "axes[0].set_title('Distance matched, excluding cross mapped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try with 3 gene clusters\n",
    "\n",
    "null_threes_genome = get_null_clusters(expressed_gencode, 3, cluster_df=None)\n",
    "\n",
    "# to make this a bit quicker\n",
    "null_threes_genome = null_threes_genome\n",
    "add_annotations(null_threes_genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_odds(cluster_df[cluster_df['N_genes']==3], null_threes_genome, ['has_bidirectional_promoter', 'has_shared_enhancer', 'has_shared_strong_enhancer', 'has_paralog', 'has_cross_map', 'has_shared_go_any', 'has_shared_go_all'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## combining over all tissues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_all_tissues = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tissue_ids = tissue_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in cluster data for a single tissue\n",
    "cluster_df = pd.read_csv(f'{prefix}/{clusters_dir}/{my_tissue_id}_clusters_all_chr.csv', index_col=0)\n",
    "\n",
    "# load in cluster data for all tissues\n",
    "if load_all_tissues:\n",
    "    all_tissue_cluster_dfs={}\n",
    "    for tissue_id in sub_tissue_ids: \n",
    "        all_tissue_cluster_dfs[tissue_id] = pd.read_csv(f'{prefix}/{clusters_dir}/{tissue_id}_clusters_all_chr.csv', index_col=0)\n",
    "\n",
    "# load in expression data (so we only compare to pairs of genes also expressed in this tissue)\n",
    "# load in gene expression data\n",
    "expression_df = pd.read_csv(f'{prefix}/{expression_dir}/{my_tissue_id}.v8.normalized_expression.bed', sep='\\t')\n",
    "\n",
    "# load in expression data for all tissues \n",
    "if load_all_tissues:\n",
    "    all_tissue_expression_dfs={}\n",
    "    for tissue_id in sub_tissue_ids: \n",
    "        all_tissue_expression_dfs[tissue_id] = pd.read_csv(f'{prefix}/{expression_dir}/{tissue_id}.v8.normalized_expression.bed', sep='\\t')\n",
    "\n",
    "# expressed genes in sample tissue\n",
    "expressed_gencode = full_gencode[full_gencode['transcript_id'].isin(expression_df['gene_id'])]\n",
    "expressed_gencode = expressed_gencode.sort_values(['chr', 'start', 'end'])\n",
    "\n",
    "\n",
    "# expressed genes in all tissues\n",
    "if load_all_tissues:\n",
    "    all_tissue_expressed_gencode_dfs={}\n",
    "    for tissue_id in sub_tissue_ids: \n",
    "        expressed_gencode = full_gencode[full_gencode['transcript_id'].isin(all_tissue_expression_dfs[tissue_id]['gene_id'])]\n",
    "        expressed_gencode = expressed_gencode.sort_values(['chr', 'start', 'end'])\n",
    "        all_tissue_expressed_gencode_dfs[tissue_id] = expressed_gencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tissue_null_distance_matched = {}\n",
    "all_tissue_nulls = {}\n",
    "\n",
    "for tissue_id in sub_tissue_ids: \n",
    "    # annotate all the clusters\n",
    "    add_annotations(all_tissue_cluster_dfs[tissue_id])\n",
    "\n",
    "    # create all the nulls\n",
    "    all_tissue_nulls[tissue_id] = get_null_clusters(all_tissue_expressed_gencode_dfs[tissue_id], 2, cluster_df=all_tissue_cluster_dfs[tissue_id])\n",
    "    annotate_sizes(all_tissue_cluster_dfs[tissue_id])\n",
    "    add_annotations(all_tissue_nulls[tissue_id])\n",
    "    all_tissue_null_distance_matched[tissue_id] = get_resamp_null_cluster(all_tissue_nulls[tissue_id], all_tissue_cluster_dfs[tissue_id][all_tissue_cluster_dfs[tissue_id]['N_genes']==2], plot=True)\n",
    "    plt.show()\n",
    "    add_annotations(all_tissue_null_distance_matched[tissue_id])\n",
    "\n",
    "\n",
    "    # write out annotated clusters and nulls for future use\n",
    "    all_tissue_cluster_dfs[tissue_id].to_csv(f'{prefix}/{clusters_dir}/{tissue_id}_clusters_annotated.csv', sep='\\t')\n",
    "    all_tissue_nulls[tissue_id].to_csv(f'{prefix}/{clusters_dir}/{tissue_id}_clusters_null.csv', sep='\\t', index=None)\n",
    "    all_tissue_null_distance_matched[tissue_id].to_csv(f'{prefix}/{clusters_dir}/{tissue_id}_clusters_null_distance_matched.csv', sep='\\t', index=None)\n",
    "\n",
    "\n",
    "\n",
    "    # make a plot for this tissue\n",
    "\n",
    "    axes = plot_log_odds(all_tissue_cluster_dfs[tissue_id][(all_tissue_cluster_dfs[tissue_id]['N_genes']==2)&(all_tissue_cluster_dfs[tissue_id]['has_cross_map']==False)], \n",
    "                         all_tissue_null_distance_matched[tissue_id][(all_tissue_null_distance_matched[tissue_id]['has_cross_map']==False)], \n",
    "                         ['has_bidirectional_promoter', 'has_shared_strong_enhancer', 'has_shared_very_strong_enhancer', 'has_paralog', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "    axes[0].set_title(f'{tissue_id} Distance matched, excluding cross mapped')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the annotated dfs\n",
    "all_tissue_null_distance_matched = {}\n",
    "all_tissue_nulls = {}\n",
    "all_tissue_cluster_dfs = {}\n",
    "\n",
    "for tissue_id in sub_tissue_ids: \n",
    "\n",
    "    # read in annotated clusters and nulls\n",
    "    all_tissue_cluster_dfs[tissue_id] = pd.read_csv(f'{prefix}/{clusters_dir}/{tissue_id}_clusters_annotated.csv', sep='\\t')\n",
    "    all_tissue_nulls[tissue_id] = pd.read_csv(f'{prefix}/{clusters_dir}/{tissue_id}_clusters_null.csv', sep='\\t')\n",
    "    all_tissue_null_distance_matched[tissue_id] = pd.read_csv(f'{prefix}/{clusters_dir}/{tissue_id}_clusters_null_distance_matched.csv', sep='\\t')\n",
    "\n",
    "\n",
    "\n",
    "    # make a plot for this tissue\n",
    "\n",
    "\n",
    "    axes = plot_log_odds(all_tissue_cluster_dfs[tissue_id][(all_tissue_cluster_dfs[tissue_id]['N_genes']==2)&(all_tissue_cluster_dfs[tissue_id]['has_cross_map']==False)], \n",
    "                         all_tissue_nulls[tissue_id][(all_tissue_nulls[tissue_id]['has_cross_map']==False)], \n",
    "                         ['has_bidirectional_promoter', 'has_shared_strong_enhancer', 'has_shared_very_strong_enhancer', 'has_paralog', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "    axes[0].set_title(f'{tissue_id} Not distance matched, excluding cross mapped')\n",
    "\n",
    "    axes = plot_log_odds(all_tissue_cluster_dfs[tissue_id][(all_tissue_cluster_dfs[tissue_id]['N_genes']==2)&(all_tissue_cluster_dfs[tissue_id]['has_cross_map']==False)], \n",
    "                         all_tissue_null_distance_matched[tissue_id][(all_tissue_null_distance_matched[tissue_id]['has_cross_map']==False)], \n",
    "                         ['has_bidirectional_promoter', 'has_shared_strong_enhancer', 'has_shared_very_strong_enhancer', 'has_paralog', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "    axes[0].set_title(f'{tissue_id} Distance matched, excluding cross mapped')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the null and clusters\n",
    "\n",
    "full_cluster_df = pd.concat([all_tissue_cluster_dfs[t] for t in sub_tissue_ids])\n",
    "full_null_df = pd.concat([all_tissue_nulls[t] for t in sub_tissue_ids])\n",
    "full_null_distance_matched_df = pd.concat([all_tissue_null_distance_matched[t] for t in sub_tissue_ids])\n",
    "\n",
    "\n",
    "axes = plot_log_odds(full_cluster_df[(full_cluster_df['N_genes']==2)&(full_cluster_df['has_cross_map']==False)], \n",
    "                         full_null_df[full_null_df['has_cross_map']==False], \n",
    "                         ['has_bidirectional_promoter', 'has_shared_strong_enhancer', 'has_paralog', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "axes[0].set_title(f'Combined tissues excluding cross-mapped')\n",
    "\n",
    "\n",
    "axes = plot_log_odds(full_cluster_df[(full_cluster_df['N_genes']==2)&(full_cluster_df['has_cross_map']==False)], \n",
    "                         full_null_distance_matched_df[full_null_distance_matched_df['has_cross_map']==False], \n",
    "                         ['has_bidirectional_promoter', 'has_shared_strong_enhancer', 'has_paralog', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "axes[0].set_title(f'Combined tissues distance matched, excluding cross-mapped')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = plot_log_odds(full_cluster_df[(full_cluster_df['has_cross_map']==False)], \n",
    "                         full_null_distance_matched_df[full_null_distance_matched_df['has_cross_map']==False], \n",
    "                         ['has_bidirectional_promoter', 'has_shared_strong_enhancer', 'has_paralog', 'has_shared_go_any', 'has_shared_go_all'])\n",
    "axes[0].set_title(f'Combined tissues excluding cross-mapped')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distance and n genes matched nulls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
